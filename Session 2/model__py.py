# -*- coding: utf-8 -*-
"""model_.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n6YU9abp8MLj6kt1eyd8auSAd5xXGla6
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
import pickle

# Define a base class for the model pipeline
class BaseModel:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()

    def load(self, filepath):
        """Load the dataset from a CSV file."""
        self.data = pd.read_csv(filepath)  # Changed to read CSV instead of Excel
        print(f"Data loaded from {filepath}")

    def preprocess(self):
        """Preprocess the dataset (e.g., handle missing values, encoding, scaling)."""
        # Handle missing values by filling with the mean of the column for numeric columns only
        numeric_cols = self.data.select_dtypes(include=['number']).columns
        self.data[numeric_cols] = self.data[numeric_cols].fillna(self.data[numeric_cols].mean())

        # Encode categorical variables (if any)
        categorical_cols = self.data.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            self.data[col] = self.label_encoder.fit_transform(self.data[col].astype(str))

        # Ensure the target column (loan_status) is categorical
        if 'loan_status' in self.data.columns:
            self.data['loan_status'] = self.data['loan_status'].astype(int)

        # Scale numerical features (excluding the target column)
        numerical_cols = self.data.select_dtypes(include=['float64', 'int64']).columns
        numerical_cols = numerical_cols.drop('loan_status', errors='ignore')  # Exclude target
        self.data[numerical_cols] = self.scaler.fit_transform(self.data[numerical_cols])

        print("Preprocessing completed.")

    def train(self, features, target):
        """Train the model and evaluate it on the validation set."""
        X = self.data[features]
        y = self.data[target]

        # Split the data into training and validation sets
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train the model
        self.model.fit(X_train, y_train)

        # Evaluate the model on the validation set
        y_pred = self.model.predict(X_val)
        print("Model training completed.")
        print(classification_report(y_val, y_pred))  # Print classification metrics
        print(f"ROC AUC Score: {roc_auc_score(y_val, y_pred)}")  # Print ROC AUC score

    def test(self, test_filepath, features, target):
        """Test the model on a separate validation dataset."""
        test_data = pd.read_csv(test_filepath)  # Changed to read CSV instead of Excel
        test_data.fillna(test_data.mean(), inplace=True)

        # Preprocessing steps for the test data (same as training)
        for col in test_data.select_dtypes(include=['object']).columns:
            test_data[col] = self.label_encoder.fit_transform(test_data[col].astype(str))

        # Scale the test data using the same scaler used during training
        test_data[features] = self.scaler.transform(test_data[features])

        # Prepare features and target for testing
        X_test = test_data[features]
        y_test = test_data[target]

        # Make predictions
        y_pred = self.model.predict(X_test)

        # Evaluate the model
        print("Testing completed.")
        print(classification_report(y_test, y_pred))  # Print classification metrics
        print(f"ROC AUC Score: {roc_auc_score(y_test, y_pred)}")  # Print ROC AUC score

    def predict(self, new_data):
        """Make predictions on new data."""
        new_data_processed = self.scaler.transform(new_data)  # Apply scaling
        predictions = self.model.predict(new_data_processed)  # Make predictions
        return predictions

    def save_model(self, filename):
        """Save the trained model to a file using pickle."""
        with open(filename, 'wb') as f:
            pickle.dump(self.model, f)
        print(f"Model saved to {filename}")

# Logistic Regression Model
class LogisticRegressionModel(BaseModel):
    def __init__(self):
        super().__init__()
        self.model = LogisticRegression(max_iter=1000)

# Random Forest Model
class RandomForestModel(BaseModel):
    def __init__(self):
        super().__init__()
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)

# Example Usage
if __name__ == "__main__":
    # Logistic Regression Example
    lr_model = LogisticRegressionModel()
    lr_model.load('cleaned_train_data.csv')  # Use your cleaned_train_data.csv file here
    lr_model.preprocess()
    lr_model.train(features=['cibil_score', 'annual_inc', 'loan_amnt', 'int_rate'], target='loan_status')

    # Save the trained logistic regression model
    lr_model.save_model('logistic_regression_model.pkl')

    # Random Forest Example
    rf_model = RandomForestModel()
    rf_model.load('cleaned_train_data.csv')  # Use your cleaned_train_data.csv file here
    rf_model.preprocess()
    rf_model.train(features=['cibil_score', 'annual_inc', 'loan_amnt', 'int_rate'], target='loan_status')

    # Save the trained random forest model
    rf_model.save_model('random_forest_model.pkl')

"""# Model Selection"""

import pandas as pd
import pickle
from sklearn.preprocessing import StandardScaler

# Load the model from a file
def load_model(filename):
    with open(filename, 'rb') as f:
        model = pickle.load(f)
    print(f"Model loaded from {filename}")
    return model

# Make predictions on new data
def make_predictions(new_data, model_filename, feature_columns):
    # Load the saved model
    model = load_model(model_filename)

    # Preprocess the new data: Apply scaling (same as during training)
    scaler = StandardScaler()

    # Ensure new data is a DataFrame with the correct column names
    new_data = pd.DataFrame(new_data, columns=feature_columns)

    # Scale the new data using the same scaler used during training
    # IMPORTANT: If you've saved the scaler during training, you should use it here.
    # For simplicity, we're initializing a new scaler, but ideally you should load it from a file.
    new_data_scaled = scaler.fit_transform(new_data)

    # Convert back to DataFrame for compatibility with the model
    new_data_scaled = pd.DataFrame(new_data_scaled, columns=feature_columns)

    # Make predictions using the loaded model
    predictions = model.predict(new_data_scaled)

    return predictions


# Example usage
if __name__ == "__main__":
    # Example new data (replace with your actual new data)
    new_data = [
        [750, 60000, 10000, 12],  # Example values
        [680, 50000, 15000, 15]
    ]

    # Define feature columns that the model expects (must match the training features)
    feature_columns = ['cibil_score', 'annual_inc', 'loan_amnt', 'int_rate']

    # Predict using Logistic Regression model
    lr_predictions = make_predictions(new_data, 'logistic_regression_model.pkl', feature_columns)
    print(f"Logistic Regression Predictions: {lr_predictions}")

    # Predict using Random Forest model
    rf_predictions = make_predictions(new_data, 'random_forest_model.pkl', feature_columns)
    print(f"Random Forest Predictions: {rf_predictions}")

from sklearn.metrics import classification_report, confusion_matrix

# Example of actual labels (replace with real values if available)
actual_labels = [1, 0]  # These should be the true labels for the new data

# Print classification report
print("Logistic Regression Classification Report:")
print(classification_report(actual_labels, lr_predictions))

print("Random Forest Classification Report:")
print(classification_report(actual_labels, rf_predictions))

# Confusion Matrix
print("Confusion Matrix for Logistic Regression:")
print(confusion_matrix(actual_labels, lr_predictions))
print("Confusion Matrix for Random Forest:")
print(confusion_matrix(actual_labels, rf_predictions))

from sklearn.ensemble import RandomForestClassifier

# Adding class weights
rf_model = RandomForestClassifier(class_weight='balanced')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Example: Generating a simple classification dataset
X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Example for Logistic Regression
lr_model = LogisticRegression()

# Perform cross-validation on Logistic Regression
lr_scores = cross_val_score(lr_model, X_train, y_train, cv=5)
print("Logistic Regression CV scores:", lr_scores)
print("Logistic Regression Mean score:", lr_scores.mean())

# Example for Random Forest
rf_model = RandomForestClassifier()

# Perform cross-validation on Random Forest
rf_scores = cross_val_score(rf_model, X_train, y_train, cv=5)
print("Random Forest CV scores:", rf_scores)
print("Random Forest Mean score:", rf_scores.mean())